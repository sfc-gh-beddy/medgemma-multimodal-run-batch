{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Distributed Multi-Modal Inference with Snowflake `run_batch`\n",
    "\n",
    "## MedGemma ECG Image Classification Demo\n",
    "\n",
    "**Key Takeaway:** This demo showcases how Snowflake's `run_batch` API enables **distributed GPU inference at scale** with just a few lines of code.\n",
    "\n",
    "### Why This Matters\n",
    "| Feature | Benefit |\n",
    "|---------|----------|\n",
    "| **Single API call** | `run_batch()` handles all distribution, scheduling, and scaling |\n",
    "| **GPU acceleration** | Automatic GPU provisioning via Snowpark Container Services |\n",
    "| **Multi-modal** | Process images + text together natively |\n",
    "| **Enterprise-ready** | Data never leaves Snowflake's secure environment |\n",
    "\n",
    "### Prerequisites\n",
    "- Kaggle account (free) - Get your API key from [kaggle.com/settings](https://www.kaggle.com/settings)\n",
    "- Database `MEDGEMMA_DEMO` with compute pool `MEDGEMMA_COMPUTE_POOL`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies & Configure Kaggle\n",
    "\n",
    "> Enter your Kaggle credentials below. Get your API key from **Settings → API → Create New Token** at [kaggle.com/settings](https://www.kaggle.com/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/tmp/whl')\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Install required packages\n",
    "session.file.get(\"@MEDGEMMA_DEMO.PUBLIC.WHL_FILE/snowflake_ml_python-1.27.0-py3-none-any.whl\", \"/tmp/whl/\")\n",
    "!pip install /tmp/whl/snowflake_ml_python-1.27.0-py3-none-any.whl --force-reinstall --quiet\n",
    "!pip install -q kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kaggle-creds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# === ENTER YOUR KAGGLE API KEY HERE ===\n",
    "KAGGLE_KEY = \"YOUR_KAGGLE_API_KEY\"  # Get from kaggle.com/settings → API → Create New Token\n",
    "# ======================================\n",
    "\n",
    "os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
    "print(\"Kaggle API key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"evilspirit05/ecg-analysis\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# Count images\n",
    "import glob\n",
    "all_images = glob.glob(f\"{dataset_path}/**/*.jpg\", recursive=True)\n",
    "all_images += glob.glob(f\"{dataset_path}/**/*.png\", recursive=True)\n",
    "print(f\"Found {len(all_images)} ECG images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-header",
   "metadata": {},
   "source": [
    "## Upload Images to Snowflake Stage\n",
    "\n",
    "> This uploads the ECG images to a Snowflake stage for batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-snowflake",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"MEDGEMMA_DEMO\"\n",
    "SCHEMA_NAME = \"PUBLIC\"\n",
    "STAGE_NAME = \"ECG_STAGE\"\n",
    "OUTPUT_STAGE_NAME = \"ECG_BATCH_OUTPUT_STAGE\"\n",
    "\n",
    "session.sql(f\"USE DATABASE {DB_NAME}\").collect()\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_NAME}\").collect()\n",
    "session.sql(f\"CREATE STAGE IF NOT EXISTS {STAGE_NAME} DIRECTORY = (ENABLE = TRUE)\").collect()\n",
    "session.sql(f\"CREATE STAGE IF NOT EXISTS {OUTPUT_STAGE_NAME} DIRECTORY = (ENABLE = TRUE)\").collect()\n",
    "\n",
    "print(f\"Using: {DB_NAME}.{SCHEMA_NAME}\")\n",
    "print(f\"Stages: {STAGE_NAME}, {OUTPUT_STAGE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-images",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_path = f\"@{DB_NAME}.{SCHEMA_NAME}.{STAGE_NAME}\"\n",
    "\n",
    "print(f\"Uploading {len(all_images)} images to {stage_path}...\")\n",
    "uploaded = 0\n",
    "for img_path in all_images:\n",
    "    rel_path = os.path.relpath(img_path, dataset_path)\n",
    "    folder_name = os.path.dirname(rel_path)\n",
    "    target = f\"{stage_path}/{folder_name}\" if folder_name else stage_path\n",
    "    session.file.put(img_path, target, auto_compress=False, overwrite=False)\n",
    "    uploaded += 1\n",
    "    if uploaded % 100 == 0:\n",
    "        print(f\"  Uploaded {uploaded}/{len(all_images)}...\")\n",
    "\n",
    "session.sql(f\"ALTER STAGE {STAGE_NAME} REFRESH\").collect()\n",
    "print(f\"Upload complete! {uploaded} images in stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Step 1: Load Model from Registry\n",
    "\n",
    "> Get the pre-registered MedGemma model from Snowflake Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "registry = Registry(session=session, database_name=DB_NAME, schema_name=SCHEMA_NAME)\n",
    "mv = registry.get_model('MEDGEMMA_4B_IT_TEST').version('V_2026_02_06__15_25_17')\n",
    "print(f\"Loaded model: {mv.model_name} v{mv.version_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prep-header",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Batch Input Data\n",
    "\n",
    "> Format ECG images as multi-modal prompts for the vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_location = f\"@{DB_NAME}.{SCHEMA_NAME}.{STAGE_NAME}\"\n",
    "files_df = session.sql(f\"LS {stage_location}\")\n",
    "files_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prep-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "ECG_CLASSIFICATION_PROMPT = \"\"\"You are a board-certified cardiologist performing a systematic ECG interpretation. Analyze this 12-lead ECG following the standard clinical approach:\n",
    "\n",
    "**STEP 1 - RATE AND RHYTHM:**\n",
    "- Calculate the heart rate. Is it bradycardic (<60), normal (60-100), or tachycardic (>100)?\n",
    "- Is the rhythm regular or irregular? \n",
    "- Is there a P wave before every QRS complex?\n",
    "- If rhythm is irregular, is it irregularly irregular (suggests atrial fibrillation)?\n",
    "\n",
    "**STEP 2 - P WAVES:**\n",
    "- Are P waves present and upright in leads I, II, aVF?\n",
    "- Is P wave morphology normal or abnormal (peaked, notched, absent)?\n",
    "\n",
    "**STEP 3 - PR INTERVAL:**\n",
    "- Is the PR interval normal (120-200ms), prolonged (heart block), or short (pre-excitation)?\n",
    "\n",
    "**STEP 4 - QRS COMPLEX:**\n",
    "- Is the QRS narrow (<120ms) or wide?\n",
    "- Are there pathological Q waves (>40ms wide or >25% of R wave amplitude)?\n",
    "- Q waves in V1-V4 suggest anterior MI\n",
    "- Q waves in II, III, aVF suggest inferior MI\n",
    "\n",
    "**STEP 5 - ST SEGMENT (CRITICAL FOR MI):**\n",
    "- Is there ST elevation? In which leads?\n",
    "  - V1-V4 elevation = anterior STEMI\n",
    "  - II, III, aVF elevation = inferior STEMI\n",
    "  - I, aVL, V5-V6 elevation = lateral STEMI\n",
    "- Is there ST depression? This may indicate ischemia or reciprocal changes.\n",
    "\n",
    "**STEP 6 - T WAVES:**\n",
    "- Are T waves upright, inverted, or hyperacute (tall, peaked)?\n",
    "- Hyperacute T waves are an early sign of MI\n",
    "- Deep symmetric T wave inversion may indicate ischemia or evolved MI\n",
    "\n",
    "**FINAL CLASSIFICATION:**\n",
    "Based on your systematic analysis above, classify this ECG as ONE of:\n",
    "- **MYOCARDIAL_INFARCTION**: If you see ST elevation, hyperacute T waves, or acute ischemic changes\n",
    "- **POST_MI**: If you see pathological Q waves with no acute ST elevation (indicates old/healed MI)\n",
    "- **ABNORMAL_HEARTBEAT**: If the rhythm is irregular, there are ectopic beats, or arrhythmia is present\n",
    "- **NORMAL**: ONLY if rate is normal, rhythm is regular sinus, no Q waves, no ST changes, normal T waves\n",
    "\n",
    "Provide your complete analysis following each step, then state your final classification.\"\"\"\n",
    "\n",
    "files_pandas = files_df.to_pandas()\n",
    "\n",
    "categories = {\n",
    "    \"normal_ecg_images\": [],\n",
    "    \"abnormal_heartbeat_ecg_images\": [],\n",
    "    \"myocardial_infarction_ecg_images\": [],\n",
    "    \"post_mi_history_ecg_images\": []\n",
    "}\n",
    "\n",
    "for _, row in files_pandas.iterrows():\n",
    "    name = row['\"name\"']\n",
    "    if \"ecg_data_new_version\" in name and name.endswith(('.jpg', '.png', '.jpeg')):\n",
    "        stage_path = f\"@{DB_NAME}.{SCHEMA_NAME}.{name}\"\n",
    "        for cat in categories.keys():\n",
    "            if cat in name:\n",
    "                categories[cat].append(stage_path)\n",
    "                break\n",
    "\n",
    "print(\"Images per category:\")\n",
    "for cat, files in categories.items():\n",
    "    print(f\"  {cat}: {len(files)}\")\n",
    "\n",
    "NUM_PER_CATEGORY = 25\n",
    "jpg_files = []\n",
    "for cat, files in categories.items():\n",
    "    sampled = random.sample(files, min(NUM_PER_CATEGORY, len(files)))\n",
    "    jpg_files.extend(sampled)\n",
    "    \n",
    "random.shuffle(jpg_files)\n",
    "print(f\"\\nSelected {len(jpg_files)} images ({NUM_PER_CATEGORY} per category)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = []\n",
    "for jpg_file in jpg_files:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a medical AI assistant specialized in ECG analysis.\"}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": ECG_CLASSIFICATION_PROMPT},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": jpg_file}},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    messages_list.append(messages)\n",
    "\n",
    "schema = [\"MESSAGES\"]\n",
    "data = [(json.dumps(m),) for m in messages_list]\n",
    "input_df = session.create_dataframe(data, schema=schema)\n",
    "print(f\"Created input DataFrame with {input_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "## Step 3: Run Distributed Batch Inference\n",
    "\n",
    "> **This is the magic** - a single `run_batch()` call processes all images on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model import JobSpec, OutputSpec, SaveMode, InputSpec\n",
    "from snowflake.ml.model.inference_engine import InferenceEngine\n",
    "\n",
    "output_location = f\"@{DB_NAME}.{SCHEMA_NAME}.{OUTPUT_STAGE_NAME}/results/\"\n",
    "\n",
    "job = mv.run_batch(\n",
    "    compute_pool=\"MEDGEMMA_COMPUTE_POOL\",\n",
    "    X=input_df,\n",
    "    input_spec=InputSpec(params={\"temperature\": 0.2, \"max_tokens\": 1024}),\n",
    "    output_spec=OutputSpec(stage_location=output_location, mode=SaveMode.OVERWRITE),\n",
    "    job_spec=JobSpec(gpu_requests=\"1\"),\n",
    "    inference_engine_options={\n",
    "        \"engine\": InferenceEngine.VLLM,\n",
    "        \"engine_args_override\": [\n",
    "            \"--max-model-len=7048\",\n",
    "            \"--gpu-memory-utilization=0.9\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Batch job started: {job}\")\n",
    "print(f\"Processing {len(jpg_files)} images on GPU...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Results\n",
    "\n",
    "> Parse model outputs and visualize classification distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = session.read.option(\"pattern\", \".*\\\\.parquet\").parquet(output_location)\n",
    "\n",
    "print(f\"Total results: {results_df.count()}\")\n",
    "results_df.show(5, max_width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "results_pandas = results_df.to_pandas()\n",
    "\n",
    "def extract_classification(choices_data):\n",
    "    try:\n",
    "        if isinstance(choices_data, str):\n",
    "            choices = json.loads(choices_data)\n",
    "        else:\n",
    "            choices = choices_data\n",
    "        \n",
    "        if choices and len(choices) > 0:\n",
    "            content = choices[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            content = content.replace(\"*\", \"\")\n",
    "            \n",
    "            patterns = [\n",
    "                r'classification is[:\\s]*(NORMAL|ABNORMAL_HEARTBEAT|MYOCARDIAL_INFARCTION|POST_MI)',\n",
    "                r'classified as[:\\s]*(NORMAL|ABNORMAL_HEARTBEAT|MYOCARDIAL_INFARCTION|POST_MI)',\n",
    "                r'the ECG is[:\\s]*(NORMAL|ABNORMAL_HEARTBEAT|MYOCARDIAL_INFARCTION|POST_MI)',\n",
    "                r'FINAL CLASSIFICATION[:\\s\\S]{0,100}(NORMAL|ABNORMAL_HEARTBEAT|MYOCARDIAL_INFARCTION|POST_MI)',\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, content, re.IGNORECASE)\n",
    "                if match:\n",
    "                    return match.group(1).upper().replace(\" \", \"_\")\n",
    "    except:\n",
    "        pass\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "results_pandas[\"CLASSIFICATION\"] = results_pandas[\"id\"].apply(extract_classification)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"         ECG CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "counts = results_pandas[\"CLASSIFICATION\"].value_counts()\n",
    "total = len(results_pandas)\n",
    "for cls, count in counts.items():\n",
    "    pct = (count / total) * 100\n",
    "    bar = \"█\" * int(pct / 5)\n",
    "    print(f\"{cls:25} {count:3} ({pct:5.1f}%) {bar}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'TOTAL':25} {total:3}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pandas.to_csv(\"/tmp/ecg_results.csv\", index=False)\n",
    "session.file.put(\"/tmp/ecg_results.csv\", f\"@{DB_NAME}.{SCHEMA_NAME}.ECG_BATCH_OUTPUT_STAGE\", auto_compress=False, overwrite=True)\n",
    "print(\"Results saved to @MEDGEMMA_DEMO.PUBLIC.ECG_BATCH_OUTPUT_STAGE/ecg_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "counts = results_pandas[\"CLASSIFICATION\"].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12', '#9b59b6', '#95a5a6']\n",
    "\n",
    "axes[0].pie(counts.values, labels=counts.index, autopct='%1.1f%%', colors=colors[:len(counts)])\n",
    "axes[0].set_title('ECG Classification Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[1].barh(counts.index, counts.values, color=colors[:len(counts)])\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_title('Classification Counts', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(counts.values):\n",
    "    axes[1].text(v + 0.5, i, str(v), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary: The Power of `run_batch`\n",
    "\n",
    "### What We Demonstrated\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Images Processed** | 100 ECG images |\n",
    "| **Model Size** | 4B parameters (MedGemma) |\n",
    "| **Input Type** | Multi-modal (image + text prompt) |\n",
    "| **Lines of Code** | ~5 lines for batch inference |\n",
    "\n",
    "### Key Benefits for Healthcare AI\n",
    "\n",
    "1. **Scale Without Complexity** - Process thousands of medical images with a single API call\n",
    "2. **Secure by Design** - Data stays in Snowflake, no external transfers required  \n",
    "3. **GPU Power On-Demand** - Automatic provisioning via Snowpark Container Services\n",
    "4. **Production-Ready** - Enterprise governance, logging, and monitoring built-in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
